# -*- coding: utf-8 -*-
"""Merging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12L1L5zf1OQ96xL3a5cUVSo3hzo-0tck7
"""

!pip install pyspark

import os
import pyspark
from google.colab import files
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create SparkSession
spark = SparkSession.builder \
    .appName("BigDataProject") \
    .config('spark.ui.port', '4050') \
    .getOrCreate()

# Define the CSV file paths
file1_path = "Hotel_Room_attributes.csv"
file2_path = "hotels_RoomPrice.csv"
file3_path = "hotel_price_min_max_Formula.csv"
file4_path = "Hotel_details.csv"


# Load the CSV files into PySpark DataFrames
df1 = spark.read.csv(file1_path, header=True, inferSchema=True)
df2 = spark.read.csv(file2_path, header=True, inferSchema=True)

df3 = spark.read.csv(file1_path, header=True, inferSchema=True)
df4 = spark.read.csv(file2_path, header=True, inferSchema=True)



# Perform the inner join on a common column(s)
joined_df = df1.join(df2, on=["hotelcode","id"], how="inner")
joined_df1 = joined_df.join(df3, on=None)
joined_df2 = joined_df1.join(df4, on=None)


# Show the result
joined_df2.show()

output_csv = joined_df.toPandas().to_csv(index=False)
with open('output_file.csv', 'w') as f:
    f.write(output_csv)
files.download('output_file.csv')

# Stop the SparkSession
spark.stop()